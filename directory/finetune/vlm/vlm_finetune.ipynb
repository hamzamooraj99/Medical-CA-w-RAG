{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsloth Installation: *ONLY FOR COLAB*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# import os\n",
    "# if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "#     !pip install unsloth\n",
    "# else:\n",
    "#     # Do this only in Colab and Kaggle notebooks! Otherwise use pip install unsloth\n",
    "#     !pip install --no-deps bitsandbytes accelerate xformers==0.0.29 peft trl triton\n",
    "#     !pip install --no-deps cut_cross_entropy unsloth_zoo\n",
    "#     !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
    "#     !pip install --no-deps unsloth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastVisionModel, is_bf16_supported\n",
    "from unsloth.trainer import UnslothVisionDataCollator\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import Qwen2VLImageProcessor\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expandable Memory Segements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen_models = {\n",
    "    \"2B\": {\n",
    "        'model': \"unsloth/Qwen2-VL-2B-Instruct-unsloth-bnb-4bit\",\n",
    "        'repo' : \"hamzamooraj99/MedQA-Qwen-2B-LoRA16\"\n",
    "    },\n",
    "    \"7B\": {\n",
    "        'model': \"unsloth/Qwen2-VL-7B-Instruct-unsloth-bnb=4bit\",\n",
    "        'repo' : \"hamzamooraj99/MedQA-Qwen-7B-LoRA16\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = qwen_models['2B']['model']\n",
    "save_repo = qwen_models['2B']['repo']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hamza\\miniconda3\\envs\\unsloth_env\\Lib\\site-packages\\unsloth_zoo\\gradient_checkpointing.py:330: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  GPU_BUFFERS = tuple([torch.empty(2*256*2048, dtype = dtype, device = f\"cuda:{i}\") for i in range(n_gpus)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.14: Fast Qwen2_Vl patching. Transformers: 4.49.0.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4080 SUPER. Num GPUs = 1. Max memory: 15.992 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.visual` require gradients\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastVisionModel.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_4bit= True,\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    max_seq_length=1024\n",
    "    \n",
    "    \n",
    ")\n",
    "model = FastVisionModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers=True, \n",
    "    finetune_attention_modules=True, \n",
    "    finetune_language_layers=True, \n",
    "    finetune_mlp_modules=True,\n",
    "\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    bias='none',\n",
    "    random_state=3407,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.image_processor = Qwen2VLImageProcessor(\n",
    "    do_resize=True,\n",
    "    size={\"height\": 256, \"width\": 256},\n",
    "    max_pixels=256*256,\n",
    "    min_pixels=224*224,\n",
    "    num_workers = 16,\n",
    "    use_fast = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data & Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ac6d220d2d54c809ce55012831040b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2409e1232544c2ba6cd7fc7c91ed629",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bbb4fbe4a15480d9281c66a9034cce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f837af3b58c480fbb5f2ef61f50a1b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de8e11519e1a4be796978d9d6de6ccba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_set = load_dataset(\"hamzamooraj99/PMC-VQA-1\", split='train')\n",
    "val_set = load_dataset(\"hamzamooraj99/PMC-VQA-1\", split='validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_conversation(sample):\n",
    "    conversation = [\n",
    "        {\"role\": \"system\",\n",
    "        \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"You are a medical image analysis assistant. Provide accurate and detailed answers to questions about medical images.\"}\n",
    "            ]\n",
    "        },\n",
    "        {\"role\": \"user\",\n",
    "        \"content\": [\n",
    "                {\"type\": \"text\", \"text\": sample['Question']},\n",
    "                {\"type\": \"image\", \"image\": sample['image']}\n",
    "            ]\n",
    "        },\n",
    "        {\"role\": \"assistant\",\n",
    "        \"content\": [\n",
    "                {\"type\": \"text\", \"text\": sample['Answer']}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    return({\"messages\": conversation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tune the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Model does not have a default image size - using 512\n"
     ]
    }
   ],
   "source": [
    "# print(\"SETTING MODEL FOR TRAINING\")\n",
    "FastVisionModel.for_training(model)\n",
    "\n",
    "# print(\"MAKING TRAINER\")\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = UnslothVisionDataCollator(model=model, processor=tokenizer, formatting_func=convert_to_conversation),\n",
    "    train_dataset = train_set,\n",
    "    eval_dataset = val_set,\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size=6,  #Each GPU processes 2 samples per batch,\n",
    "        gradient_accumulation_steps=2,  #Gradients are accumulated for 4 steps before updating model\n",
    "        warmup_steps=50,                #Gradually increases learning rate for first n steps to prevent instability\n",
    "        num_train_epochs=1,             #Parameter to perform full fine-tune (use max_steps=30 for a quick test)\n",
    "        # Optimisation & Mixed Precision\n",
    "        learning_rate=2e-4,\n",
    "        fp16=False,       #Use float16 if GPU does not support bf16\n",
    "        bf16=True,           #Use bfloat16 if GPU supports it (better stability)\n",
    "        # Optimiser & Weight Decay\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,              #Regularisation to prevent overfitting\n",
    "        lr_scheduler_type='linear',     #Decay type for learning rate from learning_rate to 0\n",
    "        seed=3407,\n",
    "        output_dir='outputs',\n",
    "        # Logging & Reporting\n",
    "        report_to='none',               #Integration with Weights & Biases ('none' disables, 'wandb' enables)\n",
    "        # Settings for Vision Fine-Tuning\n",
    "        remove_unused_columns=False,\n",
    "        dataset_text_field=\"\",\n",
    "        dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    "        dataset_num_proc=16,             #CPU processes for parallel dataset processing\n",
    "        # max_seq_length=512,\n",
    "        gradient_checkpointing = True,\n",
    "        # Validation Settings\n",
    "        do_eval=True,\n",
    "        eval_strategy='steps',\n",
    "        eval_steps=1000,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model='eval_loss',\n",
    "        greater_is_better=False,\n",
    "        per_device_eval_batch_size=4,\n",
    "        # Save Settings\n",
    "        save_strategy='steps',\n",
    "        save_steps=1000,\n",
    "        save_total_limit=2\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hamza\\miniconda3\\envs\\unsloth_env\\Lib\\site-packages\\torch\\cuda\\memory.py:391: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# @title Reset memory allocation\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "torch.cuda.reset_max_memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |   2469 MiB |   2469 MiB |   2490 MiB |  21189 KiB |\n",
      "|       from large pool |   2325 MiB |   2325 MiB |   2325 MiB |      0 KiB |\n",
      "|       from small pool |    144 MiB |    144 MiB |    164 MiB |  21189 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |   2469 MiB |   2469 MiB |   2490 MiB |  21189 KiB |\n",
      "|       from large pool |   2325 MiB |   2325 MiB |   2325 MiB |      0 KiB |\n",
      "|       from small pool |    144 MiB |    144 MiB |    164 MiB |  21189 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |   2455 MiB |   2455 MiB |   2476 MiB |  21186 KiB |\n",
      "|       from large pool |   2311 MiB |   2311 MiB |   2311 MiB |      0 KiB |\n",
      "|       from small pool |    144 MiB |    144 MiB |    164 MiB |  21186 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   2604 MiB |   2604 MiB |   2608 MiB |   4096 KiB |\n",
      "|       from large pool |   2456 MiB |   2456 MiB |   2456 MiB |      0 KiB |\n",
      "|       from small pool |    148 MiB |    148 MiB |    152 MiB |   4096 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory | 137372 KiB | 137372 KiB |    995 MiB |    860 MiB |\n",
      "|       from large pool | 133568 KiB | 133568 KiB |    843 MiB |    713 MiB |\n",
      "|       from small pool |   3804 KiB |   3804 KiB |    151 MiB |    147 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |    2423    |    2423    |    2879    |     456    |\n",
      "|       from large pool |     271    |     271    |     271    |       0    |\n",
      "|       from small pool |    2152    |    2152    |    2608    |     456    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |    2423    |    2423    |    2879    |     456    |\n",
      "|       from large pool |     271    |     271    |     271    |       0    |\n",
      "|       from small pool |    2152    |    2152    |    2608    |     456    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     190    |     190    |     192    |       2    |\n",
      "|       from large pool |     116    |     116    |     116    |       0    |\n",
      "|       from small pool |      74    |      74    |      76    |       2    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |     118    |     118    |     219    |     101    |\n",
      "|       from large pool |      79    |      79    |     113    |      34    |\n",
      "|       from small pool |      39    |      39    |     106    |      67    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 4080 SUPER. Max memory = 15.992 GB.\n",
      "2.543 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 154,253 | Num Epochs = 1 | Total steps = 15,425\n",
      "O^O/ \\_/ \\    Batch size per device = 5 | Gradient accumulation steps = 2\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (5 x 2 x 1) = 10\n",
      " \"-____-\"     Trainable parameters = 28,950,528/2,000,000,000 (1.45% trained)\n",
      "Warning: The following arguments do not match the ones in the `trainer_state.json` within the checkpoint directory: \n",
      "\tper_device_train_batch_size: 6 (from args) != 5 (from trainer_state.json)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15425' max='15425' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15425/15425 5:52:31, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.877100</td>\n",
       "      <td>0.778581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.742300</td>\n",
       "      <td>0.774723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.749400</td>\n",
       "      <td>0.772927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.857600</td>\n",
       "      <td>0.769148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.786700</td>\n",
       "      <td>0.766358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.717500</td>\n",
       "      <td>0.765929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.737700</td>\n",
       "      <td>0.764269</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but Qwen2VLForConditionalGeneration does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21164.0092 seconds used for training.\n",
      "352.73 minutes used for training.\n",
      "Peak reserved memory = 8.0 GB.\n",
      "Peak reserved memory for training = 5.457 GB.\n",
      "Peak reserved memory % of max memory = 50.025 %.\n",
      "Peak reserved memory for training % of max memory = 34.123 %.\n"
     ]
    }
   ],
   "source": [
    "# @title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dcc3ce085a644c0a36a5f75a3669911",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/644 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1812b613a49c4df7b006adfe9374033b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "652b96126f8e45aaab9f04cc43cca433",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/116M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to https://huggingface.co/hamzamooraj99/MedQA-Qwen-2B-LoRA16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ffa562275c14c7d98909cb13f39d279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/644 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef94193661a94429ad255e189123a24a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5d8c6cf8cc443c3980d819307fe2fde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.push_to_hub(save_repo)\n",
    "tokenizer.push_to_hub(save_repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hamza\\miniconda3\\envs\\unsloth_env\\Lib\\site-packages\\torch\\cuda\\memory.py:391: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# @title Reset memory allocation\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "torch.cuda.reset_max_memory_allocated()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
