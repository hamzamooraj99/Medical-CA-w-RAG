{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastVisionModel, FastLanguageModel\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import json\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_text = None\n",
    "user_image = None\n",
    "user_audio = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Text Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model_name = \"esrgesbrt/trained_health_model_llama3.1_8B_bnb_4bits\"\n",
    "text_model, text_processor = FastLanguageModel.from_pretrained(\n",
    "    text_model_name,\n",
    "    load_in_4bit=True\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(text_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Vision Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_model_name = \"hamzamooraj99/MedQA-Qwen-2B-LoRA16\"\n",
    "vision_model, vision_processor = FastVisionModel.from_pretrained(\n",
    "    vision_model_name,\n",
    "    load_in_4bit=True\n",
    ")\n",
    "\n",
    "vision_processor.image_processor.max_pixels = 512*512\n",
    "vision_processor.image_processor.min_pixels = 224*224\n",
    "\n",
    "FastVisionModel.for_inference(vision_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Whisper Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_model_name = \"openai/whisper-base\"\n",
    "speech_model = WhisperForConditionalGeneration.from_pretrained(speech_model_name)\n",
    "speech_processor = WhisperProcessor.from_pretrained(speech_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load RAAG Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "RAG_model = SentenceTransformer(RAG_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, vision_response):\n",
    "    alpaca_prompt = \"\"\" \n",
    "        Below is a query from a user regarding a medical condition or a description of symptoms. The user may also provide an image related to the query. Please provide an appropriate response to the user input with reference to the image response (if provided).\n",
    "        ### User Input:\n",
    "        {}\n",
    "\n",
    "        ### Image Response:\n",
    "        {}\n",
    "    \n",
    "        ### Response:\n",
    "        {}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = alpaca_prompt.format(text, vision_response, \"\")\n",
    "    inputs = text_processor([prompt], return_tensors=\"pt\").to('cuda')\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image, text):\n",
    "    messages = [\n",
    "        {'role': 'user',\n",
    "         'content': [\n",
    "             {'type': 'image'},\n",
    "             {'type': 'text', 'content': f\"Please describe what is shown in the image and answer the following query with reference to the image: '{text}'\"}\n",
    "         ]}\n",
    "    ]\n",
    "\n",
    "    input_text = vision_processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "    inputs = vision_processor(\n",
    "        image,\n",
    "        input_text,\n",
    "        add_special_tokens=False,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to('cuda')\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(audio_input):\n",
    "    inputs = speech_processor(audio_input, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        speech_out = speech_model.generate(**inputs)\n",
    "        transcription = speech_processor.decode(speech_out[0], skip_special_tokens=True)\n",
    "    return transcription"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transcribe Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if user_audio and not user_text:\n",
    "    user_text = transcribe_audio(user_audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Response to Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if user_image:\n",
    "    vision_inputs = preprocess_image(user_image, user_text)\n",
    "    with torch.no_grad():\n",
    "        vision_outputs = vision_model.generate(**vision_inputs, max_new_tokens=128, use_cache=True)\n",
    "        vision_response = vision_processor.decode(vision_outputs[0], skip_special_tokens=True)\n",
    "else:\n",
    "    vision_response = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate User Query and Vision Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_query(text, vision_response):\n",
    "    text = text.strip()\n",
    "    periods = ['.', '?', '!']\n",
    "    if(text[-1] not in periods):\n",
    "        text = text + \".\"\n",
    "\n",
    "    if(vision_response and vision_response.strip()):\n",
    "        return(text + \" \" + vision_response.strip())\n",
    "    \n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search FAISS Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_faiss(query, index, texts, k=5):\n",
    "    query_embedding = RAG_model.encode([query], convert_to_numpy=True).astype(\"float32\")\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    \n",
    "    return [(texts[i], distances[0][j]) for j, i in enumerate(indices[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path to RAG files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_file = r'..\\..\\dataset\\nhsInform\\faiss_index.bin'\n",
    "texts_file = r'..\\..\\dataset\\nhsInform\\texts.json'\n",
    "\n",
    "index = faiss.read_index(index_file)\n",
    "with open(texts_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    texts = json.load(f)\n",
    "\n",
    "query = embed_query(user_text, vision_response)\n",
    "results = search_faiss(query, index, texts, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Workflow Summary\n",
    "---\n",
    " 1. User provides input (text, speech, or image). ✅\n",
    " 2. Preprocessing (Whisper for speech, Qwen for images). ✅\n",
    " 3. Retrieval Step (User query is embedded & FAISS retrieves relevant texts). ✅\n",
    " 4. Augmentation Step (Relevant texts are appended to the user query).\n",
    " 5. LLaMa 3.1 generates a response based on augmented input.\n",
    " 6. TTS converts text to speech if needed.\n",
    " 7. Response is delivered to the user (as text or speech)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
