{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Evaluation Plan**\n",
    "\n",
    "1. Use MedQA for input into model (inference)\n",
    "2. Run model on the input from MedQA\n",
    "3. Compare generated response with the NHS dataset\n",
    "    - Evaluate how accurate model reflects NHS-source\n",
    "    - Run with and without RAG for two sets of evaluation\n",
    "\n",
    "---\n",
    "\n",
    "# **评估计划**\n",
    "1. 使用 MedQA 数据集作为模型的输入（推理）  \n",
    "2. 对模型运行 MedQA 输入并生成响应  \n",
    "3. 将模型生成的响应与 NHS 数据集进行比较  \n",
    "    - 评估模型输出与 NHS 知识源的相似度  \n",
    "    - 分别运行 **带有 RAG** 和 **不带 RAG** 两组评估，以对比效果  \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Thought Process**\n",
    "### Indirect knowledge validation:\n",
    "*Check model's accuracy to the NHS dataset as the source of truth. Basically, treat NHS dataset as the ground truth*\n",
    "\n",
    "### Cross-dataset accuracy:\n",
    "*Measuring accuracy of model based on NHS information while providing non-NHS-sourced questions from MedQA*\n",
    "\n",
    "### Realistic Benchmark\n",
    "*Shows whether model generalises correctly while being grounded in NHS fact.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Implementation Idea (实现思路)**\n",
    "We will run two evaluations (我们将运行两组评估):\n",
    "1. Without RAG (不使用 RAG)  \n",
    "2. With RAG (使用 RAG)  \n",
    "---  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Outline of Idea - Singular Inference**\n",
    "### **Without RAG**\n",
    "- **Query Processing:**  \n",
    "    - Pass the input query through the RAG retrieval system.  \n",
    "    - Retrieve **k** documents from FAISS search → **This is our *Ground Truth***.  \n",
    "- **Inference:**  \n",
    "    - Run inference on the query **without** the RAG results.  \n",
    "    - Receive model output.  \n",
    "- **Evaluation:**  \n",
    "    - Compare the model’s output against the **retrieved k documents** to determine the evaluation metric.  \n",
    "\n",
    "### **With RAG**\n",
    "- **Query Processing:**  \n",
    "    - Pass the input query through the RAG retrieval system.  \n",
    "    - Retrieve **k** documents from FAISS search → **This is our *Ground Truth***.  \n",
    "- **Inference:**  \n",
    "    - Combine the RAG search results with the query into a single model prompt.  \n",
    "    - Run inference on the **combined query + RAG context**.  \n",
    "    - Receive model output.  \n",
    "- **Evaluation:**  \n",
    "    - Compare the model’s output against the **retrieved k documents** to determine the evaluation metric.  \n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  \n",
    "## **单次推理评估思路**\n",
    "### **不使用 RAG**\n",
    "- **查询处理：**  \n",
    "    - 将输入查询传入 RAG 检索系统。  \n",
    "    - 从 FAISS 检索中返回 **k** 个相关文档 → **这将作为我们的 *真实值（Ground Truth）***。  \n",
    "- **推理过程：**  \n",
    "    - 在 **没有 RAG 检索结果**的情况下运行模型推理。  \n",
    "    - 获取模型输出。  \n",
    "- **评估方法：**  \n",
    "    - 将模型输出与 **检索到的 k 个文档** 进行比较，以计算评估指标。  \n",
    "\n",
    "### **使用 RAG**\n",
    "- **查询处理：**  \n",
    "    - 将输入查询传入 RAG 检索系统。  \n",
    "    - 从 FAISS 检索中返回 **k** 个相关文档 → **这将作为我们的 *真实值（Ground Truth）***。  \n",
    "- **推理过程：**  \n",
    "    - 将 RAG 检索的结果与查询合并为单个模型提示（prompt）。  \n",
    "    - 对 **合并后的查询 + RAG 上下文** 运行推理。  \n",
    "    - 获取模型输出。  \n",
    "- **评估方法：**  \n",
    "    - 将模型输出与 **检索到的 k 个文档** 进行比较，以计算评估指标。  \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Batch Evaluation Setup (批量评估设置)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset\n",
    "import faiss\n",
    "import json\n",
    "\n",
    "med_qa = load_dataset(\"MedQA\", split='test')\n",
    "\n",
    "# Load Language Model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"esrgesbrt/trained_health_model_llama3.1_8B_bnb_4bits\",\n",
    "    load_in_4bit=True\n",
    ")\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Load RAG Model\n",
    "RAG_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Query Pre-processing using Collator\n",
    "class LanguageDataCollator:\n",
    "    def __init__(self, use_rag):\n",
    "        self.use_rag = use_rag\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "\n",
    "        questions = [sample['Question'] for sample in batch]\n",
    "        retrieved_info = [self.search_faiss(question) for question in questions]\n",
    "\n",
    "        if(self.use_rag):\n",
    "            prompts = [\n",
    "                f\"\"\" \n",
    "                Below is a query from a user regarding a medical condition or a description of symptoms.\n",
    "                Please provide an appropriate response to the user input, making use of the retrieved information from our knowledge source.\n",
    "                ### User Input:\n",
    "                {question}\n",
    "\n",
    "                ### Retrieved Information\n",
    "                {info}\n",
    "            \n",
    "                ### Response:\n",
    "                {{}}\n",
    "                \n",
    "                \"\"\"\n",
    "                for question, info in zip(questions, retrieved_info)\n",
    "            ]\n",
    "            inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to('cuda')\n",
    "        else:\n",
    "            prompts = [\n",
    "                f\"\"\" \n",
    "                Below is a query from a user regarding a medical condition or a description of symptoms.  \n",
    "                Please provide an appropriate response to the user input.\n",
    "                ### User Input:\n",
    "                {question}\n",
    "            \n",
    "                ### Response:\n",
    "                {{}}\n",
    "                \n",
    "                \"\"\"\n",
    "                for question in questions\n",
    "            ]\n",
    "            inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to('cuda')\n",
    "        \n",
    "        return {'inputs': inputs, 'retrieved_info': retrieved_info}\n",
    "    \n",
    "    def search_faiss(self, query, k=5):\n",
    "        index_file = r'..\\..\\dataset\\nhsInform\\faiss_index.bin'\n",
    "        texts_file = r'..\\..\\dataset\\nhsInform\\texts.json'\n",
    "\n",
    "        index = faiss.read_index(index_file)\n",
    "        with open(texts_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            texts = json.load(f)\n",
    "        \n",
    "        query_embedding = RAG_model.encode([query], convert_to_numpy=True).astype(\"float32\")\n",
    "        distances, indices = index.search(query_embedding, k)\n",
    "        \n",
    "        return [(texts[i], distances[0][j]) for j, i in enumerate(indices[0])]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Evaluation Steps (评估步骤)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from evaluate import load\n",
    "\n",
    "# RAG Switch\n",
    "use_rag = False\n",
    "\n",
    "# Setting up DataLoader\n",
    "collator = LanguageDataCollator(use_rag)\n",
    "test_loader = DataLoader(med_qa, batch_size=8, collate_fn=collator)\n",
    "\n",
    "# Evaluation Metrics\n",
    "bleu = load('bleu')\n",
    "rouge = load('rouge')\n",
    "bertscore = load(\"bertscore\")\n",
    "\n",
    "predictions = []\n",
    "references = []\n",
    "for batch in test_loader:\n",
    "    inputs = batch['inputs']\n",
    "    retrieved_info = batch['retrieved_info']\n",
    "\n",
    "    gen_kwargs = {\n",
    "        \"max_new_tokens\": 50,\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_k\": 50\n",
    "    }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, **gen_kwargs)\n",
    "        responses = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    for response, info in zip(responses, retrieved_info):\n",
    "        reference = [i[0] for i in info]\n",
    "        references.append(reference)\n",
    "        predictions.append(response)\n",
    "\n",
    "rouge.compute(predictions, references)\n",
    "bleu.compute(predictions, references)\n",
    "# bertscore.compute(predictions, references, model_type=\"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext\")\n",
    "bertscore.compute(predictions, references, model_type=\"distilbert-base-uncased\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
