{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup OS Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ['UNSLOTH_DISABLE_AUTO_UPDATES'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastVisionModel, FastLanguageModel\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from evaluate import load\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import Qwen2VLImageProcessor\n",
    "from datasets import load_dataset\n",
    "import faiss\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmc = load_dataset(\"hamzamooraj99/PMC-VQA-1\", split='test').shuffle(seed=42).select(range(0,2500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Text Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model_name = \"esrgesbrt/trained_health_model_llama3.1_8B_bnb_4bits\"\n",
    "text_model, text_processor = FastLanguageModel.from_pretrained(\n",
    "    text_model_name,\n",
    "    load_in_4bit=True\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(text_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Vision Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_model_name = \"hamzamooraj99/MedQA-Qwen-2B-LoRA16\"\n",
    "vision_model, vision_processor = FastVisionModel.from_pretrained(\n",
    "    vision_model_name,\n",
    "    load_in_4bit=True\n",
    ")\n",
    "\n",
    "vision_processor.image_processor = Qwen2VLImageProcessor(\n",
    "    do_resize=True,\n",
    "    max_pixels=256*256,\n",
    "    min_pixels=224*224\n",
    ")\n",
    "\n",
    "FastVisionModel.for_inference(vision_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load RAG Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "RAG_model = SentenceTransformer(RAG_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGPipeline:\n",
    "    def __init__(self, user_text, vision_response, k=5):\n",
    "        faiss_path = r'C:\\Users\\hamza\\Documents\\Heriot-Watt\\Y4\\F20CA\\Medical-CA-w-RAG\\dataset\\nhsInform\\faiss_index.bin'\n",
    "        texts_json = r'C:\\Users\\hamza\\Documents\\Heriot-Watt\\Y4\\F20CA\\Medical-CA-w-RAG\\dataset\\nhsInform\\texts.json'\n",
    "        self.index = faiss.read_index(faiss_path)\n",
    "        with open(texts_json, \"r\", encoding=\"utf-8\") as f:\n",
    "            self.texts = json.load(f)\n",
    "        self.k = k\n",
    "        self.query = self.embed_query(user_text, vision_response)\n",
    "        self.results = self.search_faiss()\n",
    "        self.context = self.format_rag_context()\n",
    "\n",
    "    def embed_query(self, text: str, vision_response: str) -> str:\n",
    "        text = text.strip()\n",
    "        periods = ['.', '?', '!']\n",
    "        if(text[-1] not in periods):\n",
    "            text = text + '.'\n",
    "        if(vision_response):\n",
    "            return(text + \" \" + vision_response.strip())\n",
    "    \n",
    "        return(text)\n",
    "    \n",
    "    def search_faiss(self):\n",
    "        query_embedding = RAG_model.encode([self.query], convert_to_numpy=True).astype(\"float32\")\n",
    "        distances, indices = self.index.search(query_embedding, self.k)\n",
    "        \n",
    "        return [(self.texts[i], distances[0][j]) for j, i in enumerate(indices[0])]\n",
    "    \n",
    "    def format_rag_context(self):\n",
    "        context = \"\\n\".join([f\"Retrieved Info {i+1}: {res[0]}\" for i, res in enumerate(self.results)])\n",
    "        return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCollator:\n",
    "    def __init__(self, use_rag, vision_processor, vision_model):\n",
    "        self.use_rag = use_rag\n",
    "        self.vision_processor = vision_processor\n",
    "        self.vision_model = vision_model\n",
    "\n",
    "    def run_vision_inference(self, images, questions):\n",
    "        messages = [[\n",
    "            {'role': 'system',\n",
    "            'content': [\n",
    "                {'type': 'text', 'text': \"You are a medical imaging analyst. Your job is to firstly provide a description of the image and then answer the question provided by the user with reference to the image\"}\n",
    "                ]\n",
    "            },\n",
    "            {'role': 'user',\n",
    "            'content': [\n",
    "                {'type': 'image'},\n",
    "                {'type': 'text', 'text': f\"Please describe what is shown in the image and answer the following query with reference to the image: '{question}'\"}\n",
    "                ]\n",
    "            }\n",
    "        ] for question in questions]\n",
    "\n",
    "        input_text = self.vision_processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "\n",
    "        # Preprocessing\n",
    "        inputs = self.vision_processor(\n",
    "            images,\n",
    "            input_text,\n",
    "            add_special_tokens=False,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=True\n",
    "        ).to('cuda')\n",
    "\n",
    "        # Inference\n",
    "        with torch.no_grad():\n",
    "            gen_ids = self.vision_model.generate(**inputs, max_new_tokens=128, use_cache=True)\n",
    "            gen_ids = gen_ids[:, inputs.input_ids.shape[1]:]\n",
    "            vision_responses = vision_processor.batch_decode(gen_ids, skip_special_tokens=True)\n",
    "\n",
    "        return vision_responses\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "\n",
    "        questions = [sample['Question'] for sample in batch]\n",
    "        images = [sample['image'] for sample in batch]\n",
    "        vision_responses = self.run_vision_inference(images, questions)\n",
    "        retrieved_info = [RAGPipeline(question, vision_response).context for question, vision_response in zip(questions, vision_responses)]\n",
    "        if(self.use_rag):\n",
    "            prompts = [f\"\"\" \n",
    "                You are a medical assistant providing health information.  \n",
    "                - Use the retrieved information to **enhance the accuracy** of your response.  \n",
    "                - Do **not generate external links** unless explicitly stated by the user.  \n",
    "                - Respond clearly and concisely. \n",
    "                ### User Input:\n",
    "                {question}  \n",
    "\n",
    "                ### Image Response:\n",
    "                {vr}\n",
    "\n",
    "                ### Retrieved Information\n",
    "                {info}\n",
    "            \n",
    "                ### Response:\n",
    "                {{}}\n",
    "            \"\"\"\n",
    "                for question, vr, info in zip(questions, vision_responses, retrieved_info)\n",
    "            ]\n",
    "        else:\n",
    "            prompts = [\n",
    "                f\"\"\" \n",
    "                Below is a query from a user regarding a medical condition or a description of symptoms. The user may also provide an image related to the query. Please provide an appropriate response to the user input with reference to the image response (if provided).\n",
    "                ### User Input:\n",
    "                {question}\n",
    "\n",
    "                ### Image Response:\n",
    "                {vr}\n",
    "            \n",
    "                ### Response:\n",
    "                {{}}\n",
    "                \n",
    "                \"\"\"\n",
    "                for question, vr in zip(questions, vision_responses)\n",
    "            ]\n",
    "        \n",
    "        inputs = text_processor(prompts, return_tensors=\"pt\", padding=True, truncation=True).to('cuda')\n",
    "        return {'inputs': inputs, 'retrieved_info': retrieved_info, 'vision_responses': vision_responses}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu = load('bleu')\n",
    "rouge = load('rouge')\n",
    "bertscore = load('bertscore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_inference(dataloader):\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Running Batch Inference\"):\n",
    "            inputs = batch['inputs']\n",
    "            retrieved_info = batch['retrieved_info']\n",
    "\n",
    "            gen_kwargs = {\n",
    "                \"max_new_tokens\": 256,\n",
    "                \"do_sample\": True,\n",
    "                \"temperature\": 0.7,\n",
    "                \"top_k\": 50\n",
    "            }\n",
    "\n",
    "            \n",
    "            outputs = text_model.generate(**inputs, **gen_kwargs)\n",
    "            responses = text_processor.batch_decode(outputs, skip_special_tokens=True)\n",
    "            \n",
    "            \n",
    "            for response, info in zip(responses, retrieved_info):\n",
    "                response_start = response.find(\"### Response:\")\n",
    "                if response_start != -1:\n",
    "                    final_response = response[response_start + len(\"### Response:\"):].strip()\n",
    "                else:\n",
    "                    final_response = response.strip()\n",
    "                \n",
    "                reference = [i[0] for i in info]\n",
    "                references.append(reference)\n",
    "                predictions.append(final_response)\n",
    "        \n",
    "    return predictions, references"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_batch(predictions, references):\n",
    "    bleu_scores = bleu.compute(predictions=predictions, references=references)\n",
    "    rouge_scores = rouge.compute(predictions=predictions, references=references)\n",
    "    bert_scores = bertscore.compute(predictions=predictions, references=references, lang='en')\n",
    "\n",
    "    results = {\n",
    "        \"BLEU\": bleu_scores[\"bleu\"],\n",
    "        \"ROUGE-1\": rouge_scores[\"rouge1\"],\n",
    "        \"ROUGE-2\": rouge_scores[\"rouge2\"],\n",
    "        \"ROUGE-L\": rouge_scores[\"rougeL\"],\n",
    "        \"BERTScore\": sum(bert_scores[\"f1\"]) / len(bert_scores[\"f1\"])\n",
    "    }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    collator = DataCollator(False, vision_processor, vision_model)\n",
    "    test_loader = DataLoader(pmc, batch_size=8, collate_fn=collator, num_workers=16, persistent_workers=True)\n",
    "\n",
    "    predictions, references = batch_inference(test_loader)\n",
    "    results = eval_batch(predictions, references)\n",
    "\n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    print(pd.DataFrame([results]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
